{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple, defaultdict\n",
    "from msdm.core.mdp import TabularMarkovDecisionProcess\n",
    "from msdm.core.pomdp import TabularPOMDP\n",
    "from msdm.core.distributions import DictDistribution\n",
    "\n",
    "State = namedtuple(\"State\", \"x y heaven hell\")\n",
    "Action = namedtuple(\"Action\", \"dx dy read\")\n",
    "Observation = namedtuple(\"Observation\", \"x y heaven\")\n",
    "\n",
    "class HeavenOrHell(TabularPOMDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        coherence=.95,\n",
    "        discount_rate=.95,\n",
    "        step_cost=-1,\n",
    "        heaven_reward=50,\n",
    "        hell_reward=-50,\n",
    "        grid=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Heaven or Hell (a.k.a. information gathering) as first described by\n",
    "        [Bonet and Geffner (1998)](https://bonetblai.github.io/reports/fall98-pomdp.pdf).\n",
    "\n",
    "        A simple POMDP where the agent must gather information to figure out\n",
    "        which goal is gives a reward or punishment.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        :coherence:       The strength of the signal about which side is heaven/hell\n",
    "        :discount_rate:\n",
    "        :step_cost:       Step cost when not reading\n",
    "        :heaven_reward:\n",
    "        :hell_reward:\n",
    "        :grid:            A multiline string representing a heaven/hell configuration.\n",
    "                          `s` is the initial state,\n",
    "                          `#` are walls,\n",
    "                          `h` and `g` are potential heaven/hell locations, and\n",
    "                          `c` is where you go to learn about how to get to heaven/hell.\n",
    "                          'd' are closed doors\n",
    "                          'o' are open doors\n",
    "                          'l' are locked doors\n",
    "        \"\"\"\n",
    "        if grid is None:\n",
    "            grid = \\\n",
    "            \"\"\"\"\"\n",
    "            h...g\n",
    "            ##.##\n",
    "            .....\n",
    "            ##s.c\n",
    "            \"\"\"\"\"\n",
    "        grid = [list(r.strip()) for r in grid.split('\\n') if len(r.strip()) > 0]\n",
    "        self.grid = grid\n",
    "        self.loc_features = {}\n",
    "        self.features_loc = defaultdict(list)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                self.loc_features[(x, y)] = f\n",
    "                self.features_loc[f].append((x, y))\n",
    "        self.coherence = coherence\n",
    "        self.discount_rate = discount_rate\n",
    "        self.step_cost = step_cost\n",
    "        self.heaven_reward = heaven_reward\n",
    "        self.hell_reward = hell_reward\n",
    "\n",
    "    def initial_state_dist(self):\n",
    "        x, y = self.features_loc['s'][0]\n",
    "        return DictDistribution({\n",
    "            State(x=x, y=y, heaven='g', hell='h'): 0.5,\n",
    "            State(x=x, y=y, heaven='h', hell='g'): 0.5,\n",
    "        })\n",
    "\n",
    "    def actions(self, s):\n",
    "        return (\n",
    "            Action(0, -1, False),\n",
    "            Action(0, 1, False),\n",
    "            Action(-1, 0, False),\n",
    "            Action(1, 0, False),\n",
    "            Action(0, 0, True),\n",
    "        )\n",
    "\n",
    "    def is_absorbing(self, s):\n",
    "        loc = (s.x, s.y)\n",
    "        return self.loc_features[loc] in (s.heaven, s.hell)\n",
    "\n",
    "    def next_state_dist(self, s, a):\n",
    "        x, y = s.x, s.y\n",
    "        nx, ny = (s.x + a.dx, s.y + a.dy)\n",
    "        if self.loc_features.get((nx, ny), '#') == '#':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'l') == 'l':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'd') == 'd':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        return DictDistribution({\n",
    "            State(x=nx, y=ny, heaven=s.heaven, hell=s.hell): 1\n",
    "        })\n",
    "\n",
    "    def reward(self, s, a, ns):\n",
    "        r = 0\n",
    "        r += self.step_cost\n",
    "        if self.loc_features[(ns.x, ns.y)] == ns.heaven:\n",
    "            r += self.heaven_reward\n",
    "        elif self.loc_features[(ns.x, ns.y)] == ns.hell:\n",
    "            r += self.hell_reward\n",
    "        return r\n",
    "\n",
    "    def observation_dist(self, a, ns):\n",
    "        nloc = ns.x, ns.y\n",
    "        if a.read and (self.loc_features[nloc] == 'c'): #go to church\n",
    "            return DictDistribution({\n",
    "                Observation(x=ns.x, y=ns.y, heaven=ns.heaven): self.coherence,\n",
    "                Observation(x=ns.x, y=ns.y, heaven=ns.hell): 1 - self.coherence\n",
    "            })\n",
    "        return DictDistribution({\n",
    "                Observation(x=ns.x, y=ns.y, heaven=\" \"): 1.,\n",
    "        })\n",
    "\n",
    "    def state_string(self, s):\n",
    "        grid = copy.deepcopy(self.grid)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                if (x, y) == (s.x, s.y):\n",
    "                    grid[y][x] = '@'\n",
    "        return '\\n'.join([''.join(r) for r in grid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msdm.algorithms import  PointBasedValueIteration\n",
    "hh = HeavenOrHell(\n",
    "    coherence=.9,\n",
    "    grid=\n",
    "        \"\"\"\"\"\n",
    "        h...g\n",
    "        ##.##\n",
    "        .....\n",
    "        ##s.c\n",
    "        \"\"\"\"\",\n",
    "    discount_rate=.9\n",
    ")\n",
    "pbvi_res = PointBasedValueIteration(\n",
    "    min_belief_expansions=5,\n",
    "    max_belief_expansions=100\n",
    ").plan_on(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      ".....\n",
      "##@.c\n",
      "Action(dx=1, dy=0, read=False)\n",
      "Observation(x=3, y=4, heaven=' ')\n",
      "\n",
      "state 1: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      ".....\n",
      "##s@c\n",
      "Action(dx=1, dy=0, read=False)\n",
      "Observation(x=4, y=4, heaven=' ')\n",
      "\n",
      "state 2: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      ".....\n",
      "##s.@\n",
      "Action(dx=0, dy=0, read=True)\n",
      "Observation(x=4, y=4, heaven='g')\n",
      "\n",
      "state 3: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      ".....\n",
      "##s.@\n",
      "Action(dx=0, dy=0, read=True)\n",
      "Observation(x=4, y=4, heaven='g')\n",
      "\n",
      "state 4: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      ".....\n",
      "##s.@\n",
      "Action(dx=0, dy=-1, read=False)\n",
      "Observation(x=4, y=3, heaven=' ')\n",
      "\n",
      "state 5: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      "....@\n",
      "##s.c\n",
      "Action(dx=-1, dy=0, read=False)\n",
      "Observation(x=3, y=3, heaven=' ')\n",
      "\n",
      "state 6: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      "...@.\n",
      "##s.c\n",
      "Action(dx=-1, dy=0, read=False)\n",
      "Observation(x=2, y=3, heaven=' ')\n",
      "\n",
      "state 7: \n",
      "\"\"\n",
      "h...g\n",
      "##.##\n",
      "..@..\n",
      "##s.c\n",
      "Action(dx=0, dy=-1, read=False)\n",
      "Observation(x=2, y=2, heaven=' ')\n",
      "\n",
      "state 8: \n",
      "\"\"\n",
      "h...g\n",
      "##@##\n",
      ".....\n",
      "##s.c\n",
      "Action(dx=0, dy=-1, read=False)\n",
      "Observation(x=2, y=1, heaven=' ')\n",
      "\n",
      "state 9: \n",
      "\"\"\n",
      "h.@.g\n",
      "##.##\n",
      ".....\n",
      "##s.c\n",
      "Action(dx=1, dy=0, read=False)\n",
      "Observation(x=3, y=1, heaven=' ')\n",
      "\n",
      "state 10: \n",
      "\"\"\n",
      "h..@g\n",
      "##.##\n",
      ".....\n",
      "##s.c\n",
      "Action(dx=1, dy=0, read=False)\n",
      "Observation(x=4, y=1, heaven=' ')\n",
      "\n",
      "state 11: \n",
      "\"\"\n",
      "h...@\n",
      "##.##\n",
      ".....\n",
      "##s.c\n",
      "None\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pbvi_res.policy\n",
    "traj = pbvi_res.policy.run_on(hh)\n",
    "tuple(traj[0])\n",
    "for t, step in enumerate(traj):\n",
    "    sstr = hh.state_string(step.state)\n",
    "    print(f\"state {t}: \\n\", sstr, sep=\"\")\n",
    "    print(step.action)\n",
    "    print(step.observation)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
