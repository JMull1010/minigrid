{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple, defaultdict\n",
    "from msdm.core.mdp import TabularMarkovDecisionProcess\n",
    "from msdm.core.pomdp import TabularPOMDP\n",
    "from msdm.core.distributions import DictDistribution\n",
    "\n",
    "State = namedtuple(\"State\", \"x y\")\n",
    "Action = namedtuple(\"Action\", \"dx dy open\")\n",
    "Observation = namedtuple(\"Observation\", \"x y\")\n",
    "\n",
    "class KeysAndDoors(TabularPOMDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        coherence=.95,\n",
    "        discount_rate=.95,\n",
    "        step_cost=-1,\n",
    "        target_reward=50,\n",
    "        grid=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Heaven or Hell (a.k.a. information gathering) as first described by\n",
    "        [Bonet and Geffner (1998)](https://bonetblai.github.io/reports/fall98-pomdp.pdf).\n",
    "\n",
    "        A simple POMDP where the agent must gather information to figure out\n",
    "        which goal is gives a reward or punishment.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        :coherence:       The strength of the signal about which side is heaven/hell\n",
    "        :discount_rate:\n",
    "        :step_cost:       Step cost when not reading\n",
    "        :reward:\n",
    "        :grid:            A multiline string representing a heaven/hell configuration.\n",
    "                          `s` is the initial state,\n",
    "                          `#` are walls,\n",
    "                          't' is the target\n",
    "                          'd' are closed doors\n",
    "                          'o' are open doors\n",
    "                          'l' are locked doors\n",
    "        \"\"\"\n",
    "        if grid is None:\n",
    "            grid = \\\n",
    "            \"\"\"\n",
    "            t..d.\n",
    "            ##.##\n",
    "            .....\n",
    "            ##s..\n",
    "            \"\"\"\n",
    "        grid = [list(r.strip()) for r in grid.split('\\n') if len(r.strip()) > 0]\n",
    "        self.grid = grid\n",
    "        self.height = len(self.grid)\n",
    "        self.width = len(self.grid[0])\n",
    "        self.loc_features = {}\n",
    "        self.features_loc = defaultdict(list)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                if f == '.':\n",
    "                    self.loc_features[(x, y)] = '.'\n",
    "                else:\n",
    "                    self.loc_features[(x, y)] = f\n",
    "                self.features_loc[f].append((x, y))\n",
    "\n",
    "        self.coherence = coherence\n",
    "        self.discount_rate = discount_rate\n",
    "        self.step_cost = step_cost\n",
    "        self.target_reward = target_reward\n",
    "\n",
    "        \n",
    "\n",
    "    def initial_state_dist(self):\n",
    "        x, y = self.features_loc['s'][0]\n",
    "        return DictDistribution({\n",
    "            State(x=x, y=y): 1.0,\n",
    "        })\n",
    "\n",
    "    def actions(self, s):\n",
    "        return (\n",
    "            Action(0, -1, False),\n",
    "            Action(0, 1, False),\n",
    "            Action(-1, 0, False),\n",
    "            Action(1, 0, False),\n",
    "            Action(0, 0, True),\n",
    "        )\n",
    "\n",
    "    def is_absorbing(self, s):\n",
    "        loc = (s.x, s.y)\n",
    "        return self.loc_features[loc] == 't'\n",
    "\n",
    "    def next_state_dist(self, s, a):\n",
    "        x, y = s.x, s.y\n",
    "        nx, ny = (s.x + a.dx, s.y + a.dy)\n",
    "        adjacent = []\n",
    "\n",
    "        # Don't consider states outside of the grid\n",
    "        if nx < 0 or nx >= self.width or ny < 0 or ny >= self.height:\n",
    "            return DictDistribution({State(x=x, y=y): 1.0})\n",
    "\n",
    "        # Check if agent is on an edge\n",
    "        if (x-1 >= 0):\n",
    "            adjacent.append((x-1, y))\n",
    "        if (x+1 < self.width):\n",
    "            adjacent.append((x+1, y))\n",
    "        if (y-1 >= 0):\n",
    "            adjacent.append((x, y-1))\n",
    "        if (y+1 < self.height):\n",
    "            adjacent.append((x, y+1))\n",
    "\n",
    "        # Open Door\n",
    "        if a.open:\n",
    "            for adj in adjacent:\n",
    "                # If a door is opened\n",
    "                if self.loc_features.get(adj) == 'd':\n",
    "                    adj_x, adj_y = adj\n",
    "                    self.loc_features[adj] = 'o'\n",
    "                    self.features_loc['d'].remove(adj)\n",
    "                    self.features_loc['o'].append(adj)\n",
    "                    self.grid[adj_y][adj_x] = 'o'\n",
    "\n",
    "        # Handles movement for blocked spaces\n",
    "        if self.loc_features.get((nx, ny), '#') == '#':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'l') == 'l':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'd') == 'd':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        return DictDistribution({\n",
    "            State(x=nx, y=ny): 1.0\n",
    "        })\n",
    "\n",
    "    def reward(self, s, a, ns):\n",
    "        print(f\"Checking reward for state: {ns}, features: {self.loc_features.get((ns.x, ns.y))}\")\n",
    "        r=0\n",
    "        r += self.step_cost\n",
    "        if self.loc_features[(ns.x, ns.y)] == 't':\n",
    "            r += self.target_reward\n",
    "        return r\n",
    "\n",
    "    def observation_dist(self, a, ns):\n",
    "        return DictDistribution({\n",
    "                Observation(x=ns.x, y=ns.y): 1.0\n",
    "        })\n",
    "\n",
    "    def state_string(self, s):\n",
    "        grid = copy.deepcopy(self.grid)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                if (x, y) == (s.x, s.y):\n",
    "                    grid[y][x] = '@'\n",
    "        return '\\n'.join([''.join(r) for r in grid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting planning process...\n",
      "Error during planning: KeyError: State(x=3, y=0)\n"
     ]
    }
   ],
   "source": [
    "from msdm.algorithms import  PointBasedValueIteration\n",
    "hh = KeysAndDoors(\n",
    "    coherence=.9,\n",
    "    grid=\n",
    "        \"\"\"\n",
    "        t..d.\n",
    "        ##.##\n",
    "        .....\n",
    "        ##s..\n",
    "        \"\"\",\n",
    "    discount_rate=.9\n",
    ")\n",
    "pbvi_res = PointBasedValueIteration(\n",
    "    min_belief_expansions=1,\n",
    "    max_belief_expansions=20,\n",
    ")\n",
    "\n",
    "# Try to plan and print intermediate info\n",
    "try:\n",
    "    print(\"Starting planning process...\")\n",
    "    pbvi_res = PointBasedValueIteration(\n",
    "        min_belief_expansions=1,\n",
    "        max_belief_expansions=20\n",
    "    ).plan_on(hh)\n",
    "    print(\"Planning successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during planning: {type(e).__name__}: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PointBasedValueIteration' object has no attribute 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pbvi_res.policy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m traj \u001b[38;5;241m=\u001b[39m \u001b[43mpbvi_res\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241m.\u001b[39mrun_on(hh)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mtuple\u001b[39m(traj[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(traj):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PointBasedValueIteration' object has no attribute 'policy'"
     ]
    }
   ],
   "source": [
    "# pbvi_res.policy\n",
    "traj = pbvi_res.policy.run_on(hh)\n",
    "tuple(traj[0])\n",
    "for t, step in enumerate(traj):\n",
    "    sstr = hh.state_string(step.state)\n",
    "    print(f\"state {t}: \\n\", sstr, sep=\"\")\n",
    "    print(step.action)\n",
    "    print(step.observation)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
