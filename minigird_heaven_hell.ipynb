{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple, defaultdict\n",
    "from msdm.core.mdp import TabularMarkovDecisionProcess\n",
    "from msdm.core.pomdp import TabularPOMDP\n",
    "from msdm.core.distributions import DictDistribution\n",
    "\n",
    "State = namedtuple(\"State\", \"x y\")\n",
    "Action = namedtuple(\"Action\", \"dx dy\")\n",
    "Observation = namedtuple(\"Observation\", \"x y\")\n",
    "\n",
    "class KeysAndDoors(TabularPOMDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        coherence=.95,\n",
    "        discount_rate=.95,\n",
    "        step_cost=-1,\n",
    "        target_reward=50,\n",
    "        grid=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Heaven or Hell (a.k.a. information gathering) as first described by\n",
    "        [Bonet and Geffner (1998)](https://bonetblai.github.io/reports/fall98-pomdp.pdf).\n",
    "\n",
    "        A simple POMDP where the agent must gather information to figure out\n",
    "        which goal is gives a reward or punishment.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        :coherence:       The strength of the signal about which side is heaven/hell\n",
    "        :discount_rate:\n",
    "        :step_cost:       Step cost when not reading\n",
    "        :reward:\n",
    "        :grid:            A multiline string representing a heaven/hell configuration.\n",
    "                          `s` is the initial state,\n",
    "                          `#` are walls,\n",
    "                          't' is the target\n",
    "                          'd' are closed doors\n",
    "                          'o' are open doors\n",
    "                          'l' are locked doors\n",
    "        \"\"\"\n",
    "        if grid is None:\n",
    "            grid = \\\n",
    "            \"\"\"\n",
    "            t....\n",
    "            ##.##\n",
    "            .....\n",
    "            ##s..\n",
    "            \"\"\"\n",
    "        grid = [list(r.strip()) for r in grid.split('\\n') if len(r.strip()) > 0]\n",
    "        self.grid = grid\n",
    "        self.loc_features = {}\n",
    "        self.features_loc = defaultdict(list)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                self.loc_features[(x, y)] = f\n",
    "                self.features_loc[f].append((x, y))\n",
    "        self.coherence = coherence\n",
    "        self.discount_rate = discount_rate\n",
    "        self.step_cost = step_cost\n",
    "        self.target_reward = target_reward\n",
    "\n",
    "    def initial_state_dist(self):\n",
    "        x, y = self.features_loc['s'][0]\n",
    "        return DictDistribution({\n",
    "            State(x=x, y=y): 1.0,\n",
    "        })\n",
    "\n",
    "    def actions(self, s):\n",
    "        return (\n",
    "            Action(0, -1),\n",
    "            Action(0, 1),\n",
    "            Action(-1, 0),\n",
    "            Action(1, 0),\n",
    "            Action(0, 0),\n",
    "        )\n",
    "\n",
    "    def is_absorbing(self, s):\n",
    "        loc = (s.x, s.y)\n",
    "        return self.loc_features[loc] == 't'\n",
    "\n",
    "    def next_state_dist(self, s, a):\n",
    "        x, y = s.x, s.y\n",
    "        nx, ny = (s.x + a.dx, s.y + a.dy)\n",
    "        if self.loc_features.get((nx, ny), '#') == '#':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'l') == 'l':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        if self.loc_features.get((nx, ny), 'd') == 'd':\n",
    "            nx, ny = (s.x, s.y)\n",
    "        return DictDistribution({\n",
    "            State(x=nx, y=ny): 1.0\n",
    "        })\n",
    "\n",
    "    def reward(self, s, a, ns):\n",
    "        r = 0\n",
    "        r += self.step_cost\n",
    "        if self.loc_features[(ns.x, ns.y)] == 't':\n",
    "            r += self.target_reward\n",
    "        return r\n",
    "\n",
    "    def observation_dist(self, a, ns):\n",
    "        return DictDistribution({\n",
    "                Observation(x=ns.x, y=ns.y): 1.0\n",
    "        })\n",
    "\n",
    "    def state_string(self, s):\n",
    "        grid = copy.deepcopy(self.grid)\n",
    "        for y, row in enumerate(grid):\n",
    "            for x, f in enumerate(row):\n",
    "                if (x, y) == (s.x, s.y):\n",
    "                    grid[y][x] = '@'\n",
    "        return '\\n'.join([''.join(r) for r in grid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting planning process...\n",
      "Planning successful!\n"
     ]
    }
   ],
   "source": [
    "from msdm.algorithms import  PointBasedValueIteration\n",
    "hh = KeysAndDoors(\n",
    "    coherence=.9,\n",
    "    grid=\n",
    "        \"\"\"\n",
    "        t....\n",
    "        ##.##\n",
    "        .....\n",
    "        ##s..\n",
    "        \"\"\",\n",
    "    discount_rate=.9\n",
    ")\n",
    "pbvi_res = PointBasedValueIteration(\n",
    "    min_belief_expansions=1,\n",
    "    max_belief_expansions=20,\n",
    ")\n",
    "\n",
    "# Try to plan and print intermediate info\n",
    "try:\n",
    "    print(\"Starting planning process...\")\n",
    "    pbvi_res = PointBasedValueIteration(\n",
    "        min_belief_expansions=1,\n",
    "        max_belief_expansions=20\n",
    "    ).plan_on(hh)\n",
    "    print(\"Planning successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during planning: {type(e).__name__}: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0: \n",
      "t....\n",
      "##.##\n",
      ".....\n",
      "##@..\n",
      "Action(dx=0, dy=-1)\n",
      "Observation(x=2, y=2)\n",
      "\n",
      "state 1: \n",
      "t....\n",
      "##.##\n",
      "..@..\n",
      "##s..\n",
      "Action(dx=0, dy=-1)\n",
      "Observation(x=2, y=1)\n",
      "\n",
      "state 2: \n",
      "t....\n",
      "##@##\n",
      ".....\n",
      "##s..\n",
      "Action(dx=0, dy=-1)\n",
      "Observation(x=2, y=0)\n",
      "\n",
      "state 3: \n",
      "t.@..\n",
      "##.##\n",
      ".....\n",
      "##s..\n",
      "Action(dx=-1, dy=0)\n",
      "Observation(x=1, y=0)\n",
      "\n",
      "state 4: \n",
      "t@...\n",
      "##.##\n",
      ".....\n",
      "##s..\n",
      "Action(dx=-1, dy=0)\n",
      "Observation(x=0, y=0)\n",
      "\n",
      "state 5: \n",
      "@....\n",
      "##.##\n",
      ".....\n",
      "##s..\n",
      "None\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pbvi_res.policy\n",
    "traj = pbvi_res.policy.run_on(hh)\n",
    "tuple(traj[0])\n",
    "for t, step in enumerate(traj):\n",
    "    sstr = hh.state_string(step.state)\n",
    "    print(f\"state {t}: \\n\", sstr, sep=\"\")\n",
    "    print(step.action)\n",
    "    print(step.observation)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
